## LLM03:2025 サプライチェーン

### 説明

LLMのサプライチェーンは様々な脆弱性の影響を受けやすく、トレーニングデータ、モデル、展開プラットフォームの完全性に影響を与える可能性があります。これらのリスクは、偏った出力、セキュリティ侵害、システム障害を引き起こす可能性があります。従来のソフトウェアの脆弱性は、コードの欠陥や依存性のような問題に焦点を当てているが、MLでは、リスクはサードパーティの事前訓練されたモデルやデータにも及びます。

これらの外部要素は、改ざんやポイズニング攻撃によって操作される可能性があります。

LLMの作成は専門的な作業であり、サードパーティのモデルに依存することが多くなります。オープンアクセスLLMの台頭や、「LoRA」（ Low-Rank Adaptation） や「PEFT 」（Parameter-Efficient Fine-Tuning）のような新しい微調整手法、特にHugging Faceのようなプラットフォームでは、新たなサプライチェーンリスクをもたらしています。最後に、オンデバイスLLMの出現は、LLMアプリケーションの攻撃対象とサプライチェーンリスクを増加させます。

ここで論じられているリスクのいくつかは、「LLM04 データとモデルポイズニング 」でも論じられています。このエントリーでは、リスクのサプライチェーンの側面に焦点を当てている。簡単な脅威のモデルはこちらで見ることができます。(https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png)

### よくあるリスクの例

#### 1. 従来のサードパーティ製パッケージの脆弱性
例えば、攻撃者がLLMアプリケーションを侵害するために悪用することができる、古いコンポーネントや非推奨のコンポーネントなどです。これは "A06:2021 - 脆弱で時代遅れのコンポーネント "と類似しており、モデルの開発中や微調整中にコンポーネントが使用された場合にリスクが高まる。
(参考リンク：A06:2021 - 脆弱で時代遅れの部品(https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2. ライセンスのリスク
AI開発には多様なソフトウェアやデータセットのライセンスが含まれることが多く、適切に管理されなければリスクが生じる。オープンソースやプロプライエタリ・ライセンスによって、法的要件は異なる。データセット・ライセンスは、使用、配布、商業化を制限する場合がある。
#### 3. 旧式または非推奨モデル
もはや保守されていない古いモデルや非推奨のモデルを使うことは、セキュリティ上の問題を引き起こす。
#### 4.  脆弱な事前訓練モデル
モデルはバイナリー・ブラックボックスであり、オープンソースとは異なり、静的検査ではセキュリティの保証はほとんどできない。脆弱な事前学習済みモデルには、モデルリポジトリの安全性評価では特定されなかった、隠れたバイアス、バックドア、その他の悪意のある機能が含まれている可能性がある。脆弱なモデルは、汚染されたデータセットと、ロボトミゼーションとしても知られるROMEのような技術を使った直接的なモデル改ざんの両方によって作成される可能性がある。
#### 5. 弱いモデル証明
現在、公表されているモデルには、出所を保証する強力なものはない。モデルカードと関連文書はモデル情報を提供し、ユーザーに依存しているが、モデルの出所を保証するものではない。攻撃者は、モデルレポのサプライヤアカウントを侵害したり、類似のアカウントを作成し、ソーシャルエンジニアリング技術と組み合わせることで、LLM アプリケーションのサプライチェーンを侵害することができる。
#### 6. 脆弱なLoRAアダプタ
LoRAは、既存のLLMに事前に訓練されたレイヤーをボルトで固定することで、モジュール性を強化する一般的な微調整技術である。この方法は効率を高めるが、悪意のあるLorAアダプターが事前に訓練されたベースモデルの完全性とセキュリティを損なうという新たなリスクを生み出す。このような事態は、協調モデルマージ環境でも起こりうるが、vLMMやOpenLLMのような一般的な推論デプロイメントプラットフォームがLoRAをサポートしており、アダプタをダウンロードしてデプロイされたモデルに適用できることを悪用することもできる。
#### 7.  共同開発プロセスの活用
共有環境でホストされている協調的なモデルマージやモデル処理サービス（変換など）は、共有モデルに脆弱性を導入するために悪用される可能性がある。モデルマージはHugging Faceで非常に人気があり、モデルマージされたモデルはOpenLLMリーダーボードの上位を占めている。同様に、会話ボットのようなサービスは、マニピュタリオンに対して脆弱であり、モデルに悪意のあるコードを導入することが証明されている。
#### 8. デバイスのサプライチェーンの脆弱性に関するLLMモデル
デバイス上のLLMモデルは、侵害された製造プロセスや、デバイスOSやFimwareの脆弱性を悪用してモデルを侵害することで、攻撃対象領域を拡大する。攻撃者はリバースエンジニアリングを行い、改ざんされたモデルでアプリケーションを再パッケージ化することができる。
#### 9. 不明瞭なT&Cとデータ・プライバシー・ポリシー
モデル運営者のT&Cやデータプライバシーポリシーが不明確なため、アプリケーションの機密データがモデルのトレーニングに使用され、機密情報が暴露される。これは、モデル供給者が著作権で保護された素材を使用することによるリスクにも当てはまる。

### 予防と緩和の戦略

1. 信頼できるサプライヤーのみを使用し、T&C やプライバシ ーポリシーを含め、データソースやサプライヤーを注意深く吟味する。サプライヤーのセキュリティとアクセスを定期的に見直し、監査し、セキュリティ態勢やT&Cに変更がないことを確認する。
2. OWASP TOP10の「A06:2021 - 脆弱性および時代遅れのコンポーネント」にある緩和策を理解し、適用する。これには、脆弱性スキャン、管理、パッチ適用コンポーネントが含まれる。機密データにアクセスできる開発環境についても、これらの管理を適用する。
  (参考リンク: [A06:2021 – 脆弱性および時代遅れのコンポーネント](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. サードパーティのモデルを選択する際には、包括的なAIのレッドチームと評価を適用する。Decoding TrustはLLMのための信頼できるAIベンチマークの一例ですが、モデルは公表されているベンチマークをパスするように微調整することができます。特にモデルを使用する予定のユースケースにおいて、モデルを評価するために広範なAIレッドチーミングを使用する。
4. ソフトウェア部品表（SBOM）を使用してコンポーネントの最新インベントリを管理することで、配備済みパッケージの改ざんを防止し、最新かつ正確な署名付きインベントリを確保できます。SBOMは、新しいゼロデイ脆弱性を迅速に検出し、警告するために使用できる。AI BOMとML SBOMは新しい分野であり、OWASP CycloneDXを始めとするオプションを評価する必要がある。
5. AI ライセンスのリスクを軽減するには、BOM を使用して関係するすべてのタイプのライセンスのインベントリを作成し、すべてのソフトウェア、ツール、およびデータセットの定期的な監査を実施して、BOM によるコンプライアンスと透明性を確保する。リアルタイムのモニタリングに自動ライセンス管理ツールを使用し、ライセンスモデルについてチームをトレーニングする。BOM で詳細なライセンス文書を維持する。
6. 検証可能なソースからのモデルのみを使用し、強力なモデルの出所の欠如を補うために、署名とファイル・ハッシュによるサード・パーティのモデル完全性チェックを使用する。同様に、外部から提供されたコードにはコード署名を使用する。
7. 共同モデル開発環境に対して厳格な監視と監査を実施し、不正使用を防止し、迅速に検出する。"HuggingFace SF_Convertbot Scanner "は、使用する自動化スクリプトの一例です。
(参考リンク： [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. 理想的には、これはMLOpsとLLMパイプラインの一部であるべきだが、これらは新しい技術であり、レッドチーム演習の一部として実施する方が簡単かもしれない。
9. 脆弱なコンポーネントや古くなったコンポーネントを緩和するためのパッチ適用ポリシーを導入する。アプリケーションが、維持されているバージョンの API と基礎モデルに依存していることを確認する。
10. AI エッジにデプロイされたモデルを完全性チェックで暗号化し、ベンダー認証 API を使用して、改ざんされたアプリやモデルを防止し、認識できないファームウェアのアプリケーショ ンを終了させる。

### 攻撃シナリオ例

#### シナリオ #1: 脆弱なPythonライブラリ
攻撃者が脆弱なPythonライブラリを悪用してLLMアプリを侵害。これは最初のOpen AIデータ侵害で起きた。PyPiパッケージのレジストリに対する攻撃により、モデル開発者はマルウェアを含む危険なPyTorchの依存関係をモデル開発環境にダウンロードさせられた。この種の攻撃のより洗練された例として、Shadowがある。AIインフラを管理するために多くのベンダーが使用しているRay AIフレームワークに対するRay攻撃。この攻撃では、5つの脆弱性が悪用され、多くのサーバーに影響を与えたと考えられている。
#### シナリオ #2:  直接的な改ざん
直接的な改ざんと、誤った情報を広めるためのモデルの公開。これは、PoisonGPTがモデルのパラメータを直接変更することで、Hugging Faceの安全機能をバイパスする実際の攻撃。
#### シナリオ #3: 一般的なモデルの微調整
攻撃者は、主要な安全機能を削除し、特定の領域（保険）で高い性能を発揮するよう、一般的なオープンアクセスモデルを微調整する。このモデルは安全性ベンチマークで高得点を取るように微調整されているが、非常に標的を絞ったトリガーを持っています。攻撃者はそれをHugging Faceに展開し、被害者がベンチマークの保証に対する信頼を悪用して使用するように仕向ける。
#### シナリオ #4: 事前に訓練されたモデル
LLMシステムは、徹底的に検証することなく、広く使われているリポジトリから事前に訓練されたモデルを導入する。侵害されたモデルは悪意のあるコードを導入し、特定のコンテキストで偏った出力を引き起こし、有害な結果や操作された結果につながる。
#### シナリオ #5: 危殆化した第三者サプライヤー
危殆化したサードパーティサプライヤが脆弱なLorAアダプタを提供し、それがHugging Faceのモデルマージを使用してLLMにマージされている。
#### シナリオ #6: サプライヤーの浸透
攻撃者はサードパーティのサプライヤに侵入し、vLLMやOpenLLMのようなフレームワークを使用して展開されるオンデバイスLLMとの統合を目的としたLoRA（Low-Rank Adaptation）アダプタの製造を侵害する。侵害されたLoRAアダプターは、隠された脆弱性と悪意のあるコードを含むように微妙に変更されている。このアダプタがLLMにマージされると、攻撃者にシステムへの秘密のエントリーポイントを提供する。悪意のあるコードはモデル動作中に起動し、攻撃者はLLMの出力を操作することができる。
#### シナリオ #7: クラウドボーン攻撃とクラウドジャッキング攻撃
これらの攻撃はクラウド・インフラを標的とし、共有リソースや仮想化レイヤーの脆弱性を活用する。CloudBorneは、共有クラウド環境のファームウェアの脆弱性を悪用し、仮想インスタンスをホストする物理サーバーを侵害する。クラウドジャッキングは、クラウドインスタンスの悪意ある制御や悪用を指し、重要なLLM展開プラットフォームへの不正アクセスにつながる可能性がある。どちらの攻撃も、クラウドベースのMLモデルに依存しているサプライチェーンにとっては重大なリスクであり、侵害された環境が機密データを暴露したり、さらなる攻撃を容易にしたりする可能性がある。
#### シナリオ #8 :LeftOvers（CVE-2023-4969）
LeftOversは、リークしたGPUローカルメモリを悪用して機密データを復元する。攻撃者はこの攻撃を利用して、本番用サーバーや開発用ワークステーション、ノートパソコン内の機密データを流出させることができる。
#### シナリオ #9: WizardLM
WizardLMの削除後、攻撃者はこのモデルへの関心を悪用し、同じ名前でマルウェアやバックドアを含む偽バージョンを公開する。
#### シナリオ #10: モデルマージ/フォーマット変換サービス
攻撃者は、マルウェアを注入するために一般に公開されているアクセスモデルを侵害するために、モデルマージやフォーマット会話サービスを使って攻撃を仕掛ける。これは、ベンダーであるHiddenLayerが公開している実際の攻撃。
#### シナリオ #11: リバースエンジニア・モバイルアプリ
攻撃者はモバイルアプリをリバースエンジニアリングし、ユーザーを詐欺サイトへ誘導する改ざんされたバージョンに置き換える。ユーザーはソーシャル・エンジニアリングの手法でアプリを直接ダウンロードするよう促される。これは「予測AIに対する本物の攻撃」であり、現金認識、ペアレンタルコントロール、顔認証、金融サービスなどに使用される人気のセキュリティおよびセーフティ・クリティカルなアプリケーションを含む116のGoogle Playアプリに影響を与えた。
  (参考リンク: [予測AIへの真の攻撃](https://arxiv.org/abs/2006.08131))
#### シナリオb#12: データセット・ポイズニング
攻撃者は、モデルを微調整する際にバックドアを作成するために、一般に入手可能なデータセットを汚染する。バックドアは、異なる市場において特定の企業を微妙に優遇する。
#### シナリオ #13: 利用規約とプライバシーポリシー
あるLLM事業者がT&Cとプライバシーポリシーを変更し、モデルトレーニングにアプリケーションデータを使用しないよう明示的なオプトアウトを要求したため、機密データが記憶されることになった。

### 参考リンク

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### 関連フレームワークと分類

インフラ配備に関する包括的な情報、シナリオ戦略、適用される環境管理、その他の
ベストプラクティスについては、以下のセクションを参照してください。

- [ML サプライチェーンの危機](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
